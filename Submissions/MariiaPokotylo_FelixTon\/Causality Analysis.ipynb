{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffccf05-2668-4c41-bfa1-c23e236f5d6e",
   "metadata": {},
   "source": [
    "# VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d457b-832a-423f-bc34-704be48b7b7b",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Table of contents\n",
    "\n",
    "[0: Used packages](#abs_0)<br>\n",
    "\n",
    "[1: Data preparation](#abs_1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.a: Preparing the daily stock returns](#abs_1a)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.b: Preparing the insider trades](#abs_1b)<br>\n",
    "\n",
    "[2: Time series analysis](#abs_2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.a: Building the model](#abs_2a)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.b: Stationarity of the time series](#abs_2b)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.c: Granger-Causality](#abs_2c)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.d: Impulse Response Functions](#abs_2d)<br>\n",
    "\n",
    "[3: Causality analysis based on monthly mean S&P returns](#abs_3)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87430331-df46-4fc2-9e84-e4dc19668b11",
   "metadata": {},
   "source": [
    "In this script we investigated the causality between insider transactions and stock using approaches of time series analysis. We build several vector autoregression (VAR) between two variables each: the first one indicates the stock returns, the second one represents the insider trading transactions. Firstly, we verify that the considered time series are stationary. Then, we consider the Granger causality between the variables of the considered models. Finally, we observe the impulse response functions (IRFs) for a predetermined numbers of legs for the defined VAR models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f12aa-55a3-40d7-b587-3edab5cbaaf1",
   "metadata": {},
   "source": [
    "<a id=\"abs_0\"></a>\n",
    "<hr>\n",
    "\n",
    "## 0. Used packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31520779-1dfd-4420-ae22-afa60464d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import statistics\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.api import VAR\n",
    "import scipy\n",
    "import statsmodels.tsa.vector_ar.plotting as plotting\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "random.seed(20)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c038c1c-cd46-4ca0-bb58-c06b548f5e12",
   "metadata": {},
   "source": [
    "<a id=\"abs_1\"></a>\n",
    "<hr>\n",
    "\n",
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbc36a-14eb-4acc-9d91-5a6455b9699a",
   "metadata": {},
   "source": [
    "We consider the daily stock returns obtained from the daily adjusted closing prices from Algoseek and the insider trading transactions obtained from SEC API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d8376-d237-4521-904d-2568ba652e93",
   "metadata": {},
   "source": [
    "<a id=\"abs_1a\"></a>\n",
    "\n",
    "### 1a. Preparing the daily stock returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c9848-fccd-4748-a5e4-629a8cd82aba",
   "metadata": {},
   "source": [
    "We use the previously in Data.ipynb prepared data with daily stock prices and returns from \"close_prices_returns.csv\". As we consider the insider trading only throughout the period 01.2018-12.2023, we need to restrict the time range of returns respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca512bd-d1b0-4b5a-b9ab-c70dedcbafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the returns from the csv file\n",
    "\n",
    "returns = pd.read_csv('close_prices_returns.csv')\n",
    "returns['Date'] = pd.to_datetime(returns['Date'])\n",
    "returns = returns.set_index(['issuerTicker', 'Date'])\n",
    "returns = returns.reset_index().dropna().set_index(['issuerTicker', 'Date']) \n",
    "returns = returns.loc[pd.IndexSlice[:, '2018':], :] \n",
    "\n",
    "display(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a04e5-f2e0-4db8-a442-9adef77debb4",
   "metadata": {},
   "source": [
    "<a id=\"abs_1b\"></a>\n",
    "\n",
    "### 1b. Preparing the insider trades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e002f41-9d86-4e22-a541-f2bd5744e977",
   "metadata": {},
   "source": [
    "First, we read the insider trades data and stock price return data of the relevant ticker from the csv files provided in Data.ipynb and only consider the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46fd4f0-0d54-45ed-927c-f788acfc10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read insider trades from csv file\n",
    "\n",
    "insider_trades = pd.read_csv('insider_trades.csv')\n",
    "insider_trades = insider_trades.drop(columns=['Unnamed: 0'])\n",
    "insider_trades['periodOfReport'] = pd.to_datetime(insider_trades['periodOfReport'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a28af-9943-4e68-bcdb-259a71bde77d",
   "metadata": {},
   "source": [
    "We would like to consider the causal relation between the share returns and the corresponding insider tradings. Due to the dataset size we restrict us to the 10 shares included in S&P of the biggest weight in the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0eae84-afcc-40f7-9bc1-a4cff3fdea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the top tickers in the S&P 500 index as representative tickers\n",
    "\n",
    "top_10_tickers = ['MSFT', 'AAPL', 'NVDA', 'AMZN', 'META', 'GOOG','BRK.B', 'LLY', 'AVGO', 'TSLA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3a46d-a741-4da4-9c92-4110c05b5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get insider trades from the tickers in top_10_tickers and only consider relevant columns\n",
    "\n",
    "trades = []\n",
    "\n",
    "for element in top_10_tickers :\n",
    "    insider_trades_by_ticker = insider_trades[['periodOfReport', 'issuerTicker' , 'total']].copy()\n",
    "    insider_trades_by_ticker = insider_trades_by_ticker[insider_trades_by_ticker['issuerTicker']==element] \n",
    "    insider_trades_by_ticker['periodOfReport'] = pd.to_datetime(insider_trades_by_ticker['periodOfReport'])\n",
    "    insider_trades_by_ticker = insider_trades_by_ticker.set_index(['periodOfReport'])\n",
    "    trades.append(insider_trades_by_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cc603-73c7-4ddf-a72f-68d3b313c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the returns of tickers in top_10_tickers\n",
    "\n",
    "#returns = pd.read_csv('close_prices_returns.csv')\n",
    "#returns['Date'] = pd.to_datetime(returns['Date'])\n",
    "#returns = returns.set_index(['issuerTicker', 'Date'])\n",
    "#returns = returns.reset_index().dropna().set_index(['issuerTicker', 'Date']) \n",
    "#returns = returns.loc[pd.IndexSlice[:, '2018':], :] \n",
    "returns = returns.loc[top_10_tickers][['returns']]\n",
    "\n",
    "returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74831c96-1082-479b-a538-d531b938e4e2",
   "metadata": {},
   "source": [
    "Next, we merge for each ticker the insider trading and return data and drop all rows with NaN values or in other words with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4db69-e37c-4936-9f4d-8e8c6ff45aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(10) :\n",
    "    index = trades[i].index\n",
    "    df = pd.merge(trades[i], returns.loc[top_10_tickers[i]], how='left', left_on='periodOfReport', right_on='Date')\n",
    "    df.rename(columns = {\"total\": \"Insider trades\"}, inplace = True)\n",
    "    df = df.set_index(index).dropna(axis=0)\n",
    "    df = df.drop(columns=['issuerTicker'])\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18c15a-3efd-4acf-9322-a2414b30692a",
   "metadata": {},
   "source": [
    "Delete the sixth element in the list because there is only data for one day. Also delete the corresponding ticker from top_10_tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516cf5fc-1ca3-40d8-bd76-e55fbbf16f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869388f-2ec7-4a97-be39-e76ecde73191",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5cbb8-0efe-4a9a-858a-3880d913c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del top_10_tickers[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39531070-c9e8-47c7-9ec4-617578e97208",
   "metadata": {},
   "source": [
    "Before the application of VAR, we need to check whether the data is stationary as this is one of the conditions that needs to be fulfilled for VAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992967fc-d3f3-4ae8-9eb2-5b8e26f76ba0",
   "metadata": {},
   "source": [
    "<a id=\"abs_2\"></a>\n",
    "<hr>\n",
    "\n",
    "## 2. Time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8db7ce-34e3-49ea-8971-d9e7b714fde4",
   "metadata": {},
   "source": [
    "<a id=\"abs_2a\"></a>\n",
    "\n",
    "### 2a. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d471f8-d233-4e37-affb-e93a7d9e2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {} \n",
    "insider_trades_metrics = [\"Total value\"]\n",
    "\n",
    "lags =  range(7)\n",
    "signif = 0.05\n",
    "num_tickers = 9\n",
    "counters = []\n",
    "\n",
    "for ticker in range(num_tickers):\n",
    "        for metric in insider_trades_metrics:\n",
    "            counter = 0 \n",
    "            for lag in lags:\n",
    "                model = VAR(data[ticker]).fit(maxlags=lag, ic='aic')\n",
    "                if model.k_ar == lag or model.k_ar not in lags:\n",
    "                    models[str(top_10_tickers[ticker]) + \"_\" + str(model.k_ar) + str(\"lags_\") +metric] = model\n",
    "                    counter = counter + 1 \n",
    "            counters.append(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e2d31-d474-4015-a322-8d860f6d91bd",
   "metadata": {},
   "source": [
    "<a id=\"abs_2b\"></a>\n",
    "\n",
    "### 2b. Stationarity of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a498aa-4485-4ca9-ba56-44b800d0e28f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data is stationary - One of the conditions for AR/VAR is fulfilled \n",
    "\n",
    "signif = 0.05\n",
    "lags =  range(7)\n",
    "\n",
    "for j in range(9):\n",
    "    print('\\n Ticker: ' + top_10_tickers[j])\n",
    "    for name, column in data[j][[\"returns\"]].items():\n",
    "        for i in lags:\n",
    "            r = adfuller(column, maxlag=i,regression='ct',autolag = None)\n",
    "            output = {'test_statistic': round(r[0], 4),\n",
    "                      'pvalue': round(r[1], 4),\n",
    "                      'n_lags': round(r[2], 4),\n",
    "                      'n_obs': r[3]}\n",
    "            p_value = output['pvalue']\n",
    "\n",
    "            # Print Summary\n",
    "            print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "            print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
    "            print(f' Significance Level    = {signif}')\n",
    "            print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "            print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "            print(f' Critical value {(str(round(signif*100))+\"%\").ljust(6)} = {round(r[4][str(round(signif*100))+\"%\"], 3)}')\n",
    "\n",
    "            if p_value <= signif:\n",
    "                print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "                print(f\" => Series is Stationary.\")\n",
    "            else:\n",
    "                print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "                print(f\" => Series is Non-Stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5017020-f48e-476e-b30b-41c1202faa12",
   "metadata": {},
   "source": [
    "The data is stationary and we can continue with the Granger-Causality test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942ccce-1b53-4b01-ad73-b5d9ab8d2c1f",
   "metadata": {},
   "source": [
    "<a id=\"abs_2c\"></a>\n",
    "\n",
    "### 2c. Granger-Causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d9a81-1bda-446b-9cf5-9d738df6c0e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def granger_causality(results, column_names = ['Insider trades', 'returns']):\n",
    "        test = results.test_causality(column_names[0], [column_names[1]], kind='Wald')\n",
    "        print(\"\\t \" + str(column_names[1]) + \" -> \" + str(column_names[0]) + \":\", test)\n",
    "        # Print summary of results\n",
    "        #print(results.summary())\n",
    "    \n",
    "        test = results.test_causality(column_names[1], [column_names[0]], kind='Wald')\n",
    "        print(\"\\t \" + str(column_names[0]) + \" -> \" + str(column_names[1]) + \":\", test)\n",
    "        # Print summary of results\n",
    "        #print(results.summary())\n",
    "\n",
    "# Fit the VAR model\n",
    "for model in models.keys():\n",
    "    if model.split(\"_\")[1] != \"0lags\":\n",
    "        print('\\n Model: ' + model)\n",
    "        #Granger Causality test\n",
    "        granger_causality(models[model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df137c93-901c-4be0-afdb-7dbe5e9d19e8",
   "metadata": {},
   "source": [
    "<a id=\"abs_2d\"></a>\n",
    "\n",
    "### 2d. Impulse Response Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91d890-a0c5-4e0e-9fc1-dfa2a561f282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orth = False\n",
    "repl=1000\n",
    "signif=0.05\n",
    "seed=None\n",
    "stderr_type='asym'\n",
    "plot_stderr=True\n",
    "plot_params={'font' : 20} \n",
    "subplot_params=None\n",
    "figsize = (20,7)\n",
    "plt.rc('font', size=20)\n",
    "title = \"\"\n",
    "\n",
    "for model in models.keys():\n",
    "    if model.split(\"_\")[1] != \"0lags\":\n",
    "        print(\"Model: \", model)\n",
    "        irf = models[model].irf(20)\n",
    "        \n",
    "        cum_effects = irf.cum_effects\n",
    "        lr_effects = irf.lr_effects\n",
    "        \n",
    "        \n",
    "        stderr = irf.cum_effect_cov(orth=False)\n",
    "        \n",
    "        for k in range(2):\n",
    "            impulse=1-k\n",
    "            response=k\n",
    "            fig = plotting.irf_grid_plot(cum_effects, stderr, impulse, response, \n",
    "                                         irf.model.names, title, signif=signif,\n",
    "                                         hlines=lr_effects,\n",
    "                                         subplot_params=subplot_params,\n",
    "                                         plot_params=plot_params,\n",
    "                                         figsize=figsize,\n",
    "                                         stderr_type=stderr_type)\n",
    "        \n",
    "        \n",
    "        \n",
    "            if k==0:\n",
    "                plt.title(\"Accumulated IRF's from VAR(\"+str(i)+\") Model \\n Response of %\"u\"Δ monthly returns to Innovations in %\"u\"ΔNNI\" )\n",
    "            else:\n",
    "                plt.title(\"Accumulated IRF's from VAR(\"+str(i)+\") Model \\n Response of %\"u\"ΔNNI to Innovations in %\"u\"Δ'monthly returns'\" )\n",
    "            plt.xticks(np.arange(0, 21, 1))\n",
    "            plt.grid(visible=True, axis='y')\n",
    "            plt.show()\n",
    "        del irf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514b4ee-9bb1-4d2b-8619-6f6a7708b9fb",
   "metadata": {},
   "source": [
    "<a id=\"abs_2\"></a>\n",
    "<hr>\n",
    "\n",
    "## 3. Causality analysis based on monthly mean S&P returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd81256-56ba-4bcf-8352-d84202e63d06",
   "metadata": {},
   "source": [
    "Similarly to Paragraph 2 we conduct the causality analysis based on monthly mean S&P returns and the insider trading transactions, where we build metrics for the later following the paper https://www.sciencedirect.com/science/article/pii/S1062976901001144?ref=cra_js_challenge&fr=RR-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb19ca-08dd-4615-9666-8eb928199046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and print the stock tickers that make up S&P500\n",
    "tickers = pd.read_html(\n",
    "    'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "# Get the list of tickers\n",
    "sp500_tickers = tickers['Symbol'].tolist()\n",
    "\n",
    "# Filter the returns dataframe\n",
    "filtered_returns = returns.loc[returns.index.get_level_values('issuerTicker').isin(sp500_tickers)]\n",
    "\n",
    "# Group by Date and sum up the returns for each day\n",
    "monthly_mean_returns = filtered_returns.groupby([filtered_returns.index.get_level_values(1).year, filtered_returns.index.get_level_values(1).month])['returns'].mean()\n",
    "monthly_mean_returns.index.names =[\"Year\", \"Month\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ba5ab-eaa0-4b8f-aa9a-c842883d8314",
   "metadata": {},
   "source": [
    "We build metrics for the number of insider transaction and number of acquired/disposed shares as in the following paper https://www.sciencedirect.com/science/article/pii/S1062976901001144?ref=cra_js_challenge&fr=RR-1. The metrics take into account the difference in \"sign\" of share aquistion and disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18ff81-1913-4ad8-b38e-b74e2e317d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate insider trades from csv file\n",
    "#P\u0004aggregate number of insider purchase transactions in a given month; aggregate number of shares purchased by insiders in a given month\n",
    "#S\u0004aggregate number of insider sale transactions in a given month; aggregate number of shares sold by insiders in a given month\n",
    "#NNI net number index\n",
    "#NSI net share index\n",
    "#PNI insider purchase index \n",
    "#SNI insider sale index \n",
    "#https://pdf.sciencedirectassets.com/272067/1-s2.0-S1062976900X00208/1-s2.0-S1062976901001144/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIA9Jm0431VRhla%2FynGiIVi78EI4R3uHytcvPrzcyGXu8AiEAgxODzglSJlVhjV2oElvIg%2F2n4WgKYPVBHtWNGEL18FUqvAUIiP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDHikV2glUujL3I4gfCqQBfB661ooUJN9B8iroM4TP8XA4Z26a2YCl1PXn%2B0tA3W4FX4h0t4OSGTu7bgYUI%2B9Jl6EBeQUWH1vuCGmveiToodhkr8KwDq%2FcrEiZhh%2F%2FQjbk%2BascW9pa0O2FR6ZlJmWD8pnIca1fFrAnLdnNqyuFU%2FY2bHmdiUGymEMqg%2BbJbb8qUIlbEJ8SE5XvySJTwSo4KJAX00hOxAVkePVQveuyAJLb4oGTz%2FMGIzo58WlvVjsLK2jKfg78J%2B3QZorWoFoQOsmIU6s2sSMmPE8xpxHCOJKCmt26QqlpY%2BZ5R0FvsHIcNoDEsq9z7a6gE0YjEIi5xTqz1BH6A6gWXjcFlkd%2BWzPdoq2mC2VeFSdGQdTkoxBaWkuEiPQ8WJmxiUHYNtyZDyslt6%2BTwgGp%2FPFjCpSVSu0sDzcjFT%2BmNKBc%2BmEl9NTRN5xnjDaFCs6ainQEuunwdyoqQW%2F8jq0TruRZz3bZH9A4%2FSecQkIrQQcwxwiq09lHO9AMyXAq5Xwn9xdVjjJFFTTSrZDUqtT5hQxIr8cBZcBuDWfJRoJ45LHQ%2Fh%2FVQD3gctytBYX%2BaV3Qxqjalzm7k1CNW43Ofxat1K5Bwa%2BImxnj6KGki0ZeXCqO%2FWFViRmKXxHvj%2FDsISqHosMDLEcmhrBgXNa2Z0FBcKpheEsWkIa6KKy%2Fyu4l4ToUWV7Cz9cWhyNfQojSZ1DcY5atyE8NK97sB3JojnZ03A%2FWp8ACVLGs4Gi%2BoUONKCfUUy5Sd1OpA2zquxEAVlDEQ7kRSZiSVEMrAw8KrvVeHG90XA5lZ0q0NA%2BJIcFXkmTUe%2FNMNaQGG%2FRkuxFopiKhhlJBlyS5fWMy33PXQjnYKSOfgHPCBXU1b4NQE5Kmx6UGk5Ls1bRMNnAyLQGOrEBi22DAjzLNtdOwRagWsSgy1%2BO9VVgUx22UvG7PgPE5Ot1FWkpyPhRklLeobrQnOTjtrC4ITF7tDvRQEg8nexFa1cDdQiBl26KFJ1d6WkVDZU6o6DaFEzhqY8vTdT8yAfeihHTvVcCuohojbnkEBntPohHn2XjoLmmtpi6A8mWaM16uhlSedSUvq0kuAOFfd0X8EERRpcmr6v%2FNJYtCvmwWbNmW2sILHNUiN6QayNjXN3t&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240713T071608Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYZ3B5UOG7%2F20240713%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=cbb7ad2625ca7ac14752e0e4a08b24ad492583283ae216a53c3376b64b101ed2&hash=52958844bc837c5efbec429827cd475c1a3041d26c2d1739a1da568c4e7adfa2&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1062976901001144&tid=spdf-518aba43-0dad-40d7-8d28-a2d4b950f59f&sid=2e67adb8670c474b520a492-2525918a92b3gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=130e5e06525251025500&rr=8a277c5e08b91da4&cc=ua\n",
    "\n",
    "insider_trades[\"transactions\"]=1\n",
    "insider_trades = insider_trades.loc[insider_trades['issuerTicker'].isin(sp500_tickers)]\n",
    "    \n",
    "acquired = insider_trades.loc[insider_trades[\"acquiredDisposed\"]==\"A\"]\n",
    "acquired.set_index('periodOfReport', inplace = True)\n",
    "P = acquired.groupby([acquired.index.year, acquired.index.month]).sum()[[\"transactions\",\"shares\"]]\n",
    "#P.set_index(P.index.month, inplace = True)\n",
    "disposed = insider_trades.loc[insider_trades[\"acquiredDisposed\"]==\"D\"]\n",
    "disposed.set_index('periodOfReport', inplace = True)\n",
    "S = disposed.groupby([disposed.index.year, disposed.index.month]).sum()[[\"transactions\",\"shares\"]]\n",
    "#S.set_index(S.index.month, inplace = True)\n",
    "NNI = ((P-S)/ (P+S))[\"transactions\"]\n",
    "NSI = ((P-S)/ (P+S))[\"shares\"]\n",
    "PNI = (P/ (P+S))[\"transactions\"]\n",
    "SNI = (S/ (P+S))[\"transactions\"]\n",
    "    \n",
    "NNI.rename(\"NNI\", inplace = True)\n",
    "NSI.rename(\"NSI\", inplace = True)\n",
    "PNI.rename(\"PNI\", inplace = True)\n",
    "SNI.rename(\"SNI\", inplace = True)\n",
    "\n",
    "NNI.index.names =[\"Year\", \"Month\"]\n",
    "NSI.index.names =[\"Year\", \"Month\"]\n",
    "PNI.index.names =[\"Year\", \"Month\"]\n",
    "SNI.index.names =[\"Year\", \"Month\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a4fd8-cce8-438d-8810-941962674a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "insider_trades_metrics = [\"NNI\", \"NSI\", \"PNI\", \"SNI\"]\n",
    "insider_trades_metrics_ = [NNI, NSI, PNI, SNI]\n",
    "for metric in insider_trades_metrics_:\n",
    "    df =  pd.merge(metric, monthly_mean_returns, how='left', left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"])\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91c24e-4b4a-4331-8ae2-e238577e5539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data is stationary - One of the conditions for AR/VAR is fulfilled \n",
    "\n",
    "signif = 0.05\n",
    "lags =  range(7)\n",
    "\n",
    "for df in dfs:\n",
    "    print(\"We consider \"+ str(df.columns[0]) + \" as a metric\")\n",
    "    for i in lags:\n",
    "        for name, column in df.items():\n",
    "                r = adfuller(column, maxlag=i,regression='ct',autolag = None)\n",
    "                output = {'test_statistic': round(r[0], 4),\n",
    "                          'pvalue': round(r[1], 4),\n",
    "                          'n_lags': round(r[2], 4),\n",
    "                          'n_obs': r[3]}\n",
    "                p_value = output['pvalue']\n",
    "    \n",
    "                # Print Summary\n",
    "                print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "                print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
    "                print(f' Significance Level    = {signif}')\n",
    "                print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "                print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "    \n",
    "                print(f' Critical value {(str(round(signif*100))+\"%\").ljust(6)} = {round(r[4][str(round(signif*100))+\"%\"], 3)}')\n",
    "    \n",
    "                if p_value <= signif:\n",
    "                    print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "                    print(f\" => Series is Stationary.\")\n",
    "                else:\n",
    "                    print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "                    print(f\" => Series is Non-Stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5b347-2769-4395-b2d4-bd1e271ad23c",
   "metadata": {},
   "source": [
    "We build the models with the number of lags, so that the time series preserves the stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b5480-1ce2-400d-abc3-6c5aaba3536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_fitted = [VAR(dfs[0]).fit(maxlags = 3),\n",
    "                 VAR(dfs[1]).fit(maxlags = 3),\n",
    "                 VAR(dfs[2]).fit(maxlags = 1),\n",
    "                 VAR(dfs[3]).fit(maxlags = 1) ] \n",
    "for i, model_fitted in enumerate(models_fitted):\n",
    "    granger_causality(model_fitted, [insider_trades_metrics[i], \"returns\"])\n",
    "    \n",
    "for model in models_fitted:\n",
    "    irf = model_fitted.irf(20)\n",
    "            \n",
    "    cum_effects = irf.cum_effects\n",
    "    lr_effects = irf.lr_effects\n",
    "            \n",
    "            \n",
    "    stderr = irf.cum_effect_cov(orth=False)\n",
    "            \n",
    "    for k in range(2):\n",
    "                impulse=1-k\n",
    "                response=k\n",
    "                fig = plotting.irf_grid_plot(cum_effects, stderr, impulse, response, \n",
    "                                             irf.model.names, title, signif=signif,\n",
    "                                             hlines=lr_effects,\n",
    "                                             subplot_params=subplot_params,\n",
    "                                             plot_params=plot_params,\n",
    "                                             figsize=figsize,\n",
    "                                             stderr_type=stderr_type)\n",
    "            \n",
    "            \n",
    "            \n",
    "                if k==0:\n",
    "                    plt.title(\"Accumulated IRF's from VAR(3) Model \\n Response of %\"u\"Δ monthly returns to Innovations in %\"u\"ΔNNI\" )\n",
    "                else:\n",
    "                    plt.title(\"Accumulated IRF's from VAR(3) Model \\n Response of %\"u\"ΔNNI to Innovations in %\"u\"Δ'monthly returns'\" )\n",
    "                plt.xticks(np.arange(0, 21, 1))\n",
    "                plt.grid(visible=True, axis='y')\n",
    "                plt.show()\n",
    "    del irf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
